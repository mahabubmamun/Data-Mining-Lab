{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5989467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719ac0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in the dataset: ['<=50K' '<=50K.' '>50K' '>50K.']\n",
      "Total dataset size: 48842\n",
      "Training data size: 39073\n",
      "Test data size: 9769\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(2)\n",
    "used_datasets = {\n",
    "    \"iris\": 53,\n",
    "    \"heart_disease\": 45,\n",
    "    \"wine_quality\": 186,\n",
    "    \"breast_cancer\": 17,\n",
    "    \"adult\": 2,\n",
    "    \"bank_marketing\": 222,\n",
    "    \"student_performance\": 320,\n",
    "    \"wine\": 109,\n",
    "    \"air_quality\": 360,\n",
    "    \"mushroom\": 73\n",
    "}\n",
    "iris = fetch_ucirepo(id=2)\n",
    "# print(iris)\n",
    "# data (as pandas dataframes)\n",
    "X = iris.data.features\n",
    "y = iris.data.targets\n",
    "\n",
    "# Concatenate features and targets for shuffling and splitting\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "dataset = data.to_numpy()\n",
    "# print(dataset)\n",
    "\n",
    "print(\"Classes in the dataset:\", np.unique(y))\n",
    "\n",
    "# Shuffle the dataset\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "train_split = 0.8\n",
    "train_size = int(train_split * len(dataset))\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba69510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            # Filter X and y for the current class\n",
    "            X_cls = X[y == cls]\n",
    "            # print(X_cls)\n",
    "            # Calculate mean and variance for each feature for the current class\n",
    "            # Adding a small epsilon to variance to prevent division by zero\n",
    "            self.parameters[cls] = {\n",
    "                'mean': X_cls.mean(axis=0),\n",
    "                'variance': X_cls.var(axis=0) + 1e-6\n",
    "            }\n",
    "        \n",
    "        self.class_priors = {cls: np.sum(y == cls) / len(y) for cls in self.classes}\n",
    "\n",
    "    def gauss_pdf(self, X_row, mean, variance):\n",
    "        exponent = -((X_row - mean)**2) / (2 * variance)\n",
    "        return (1 / np.sqrt(2 * np.pi * variance)) * np.exp(exponent)\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        posteriors = []\n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.class_priors[cls])\n",
    "            \n",
    "            # Ensure x and parameters are float for calculation\n",
    "            likelihood = np.sum(np.log(self.gauss_pdf(x.astype(float), self.parameters[cls]['mean'], self.parameters[cls]['variance'])))\n",
    "            \n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self.predict_single(x) for x in X]\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc07bcf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Private'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Separate features and labels from the full dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train = \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# print(X_train)\u001b[39;00m\n\u001b[32m      4\u001b[39m y_train = train_data[:, -\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Private'"
     ]
    }
   ],
   "source": [
    "# Separate features and labels from the full dataset\n",
    "X_train = train_data[:, :-1].astype(float) \n",
    "# print(X_train)\n",
    "y_train = train_data[:, -1]\n",
    "X_test = test_data[:, :-1].astype(float)   \n",
    "y_test = test_data[:, -1]\n",
    "\n",
    "# Instantiate and train the model\n",
    "classifier = NaiveBayes()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fcc238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def error_rate(y_true, y_pred):\n",
    "    return 1 - accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdda8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision_recall_fscore(y_true, y_pred, average='weighted', beta=1.0):\n",
    "    classes = np.unique(y_true)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    fscore_scores = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        true_positives = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        false_positives = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        false_negatives = np.sum((y_pred != cls) & (y_true == cls))\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        \n",
    "        fscore = ((1 + beta**2) * precision * recall) / ((beta**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        fscore_scores.append(fscore)\n",
    "        \n",
    "    if average == 'weighted':\n",
    "        weights = [np.sum(y_true == cls) for cls in classes]\n",
    "        total_samples = len(y_true)\n",
    "        \n",
    "        avg_precision = np.sum(np.array(precision_scores) * np.array(weights)) / total_samples\n",
    "        avg_recall = np.sum(np.array(recall_scores) * np.array(weights)) / total_samples\n",
    "        avg_fscore = np.sum(np.array(fscore_scores) * np.array(weights)) / total_samples\n",
    "        \n",
    "        return avg_precision, avg_recall, avg_fscore\n",
    "    else:\n",
    "        return np.mean(precision_scores), np.mean(recall_scores), np.mean(fscore_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a78f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred, average='weighted'):\n",
    "    classes = np.unique(y_true)\n",
    "    specificity_scores = []\n",
    "\n",
    "    for cls in classes:\n",
    "        true_negatives = np.sum((y_pred != cls) & (y_true != cls))\n",
    "        false_positives = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        \n",
    "        specificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "        specificity_scores.append(specificity)\n",
    "        \n",
    "    if average == 'weighted':\n",
    "        weights = [np.sum(y_true == cls) for cls in classes]\n",
    "        total_samples = len(y_true)\n",
    "        return np.sum(np.array(specificity_scores) * np.array(weights)) / total_samples\n",
    "    else:\n",
    "        return np.mean(specificity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cee095e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m acc = accuracy(\u001b[43my_test\u001b[49m, y_pred)\n\u001b[32m      2\u001b[39m err_rate = error_rate(y_test, y_pred)\n\u001b[32m      3\u001b[39m precision, recall, f1_score = precision_recall_fscore(y_test, y_pred, beta=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_test, y_pred)\n",
    "err_rate = error_rate(y_test, y_pred)\n",
    "precision, recall, f1_score = precision_recall_fscore(y_test, y_pred, beta=1.0)\n",
    "f2_score = precision_recall_fscore(y_test, y_pred, beta=2.0)[2]\n",
    "f0_5_score = precision_recall_fscore(y_test, y_pred, beta=0.5)[2]\n",
    "specificity = specificity_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fc49f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy of the multi-class decision tree: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43macc\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecision (weighted): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the multi-class decision tree: {acc:.2f}\")\n",
    "print(f\"Error Rate: {err_rate:.2f}\")\n",
    "print(f\"Precision (weighted): {precision:.2f}\")\n",
    "print(f\"Recall (weighted): {recall:.2f}\")\n",
    "print(f\"F1-Score (weighted): {f1_score:.2f}\")\n",
    "print(f\"F2-Score (weighted, beta=2.0): {f2_score:.2f}\")\n",
    "print(f\"F0.5-Score (weighted, beta=0.5): {f0_5_score:.2f}\")\n",
    "print(f\"Specificity (weighted): {specificity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d5b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Test Predictions ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- All Test Predictions ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mX_test\u001b[49m)):\n\u001b[32m      3\u001b[39m     sample_x = X_test[i]\n\u001b[32m      4\u001b[39m     sample_y_true = y_test[i]\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- All Test Predictions ---\")\n",
    "for i in range(len(X_test)):\n",
    "    sample_x = X_test[i]\n",
    "    sample_y_true = y_test[i]\n",
    "    sample_y_pred = y_pred[i]\n",
    "    #if (sample_y_pred!=sample_y_true):\n",
    "    print(f\"Sample {i+1}: Predicted='{sample_y_pred}', Actual='{sample_y_true}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44dd1f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnaive_bayes_results.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Write performance metrics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy of the Naive Bayes classifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43macc\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecision (weighted): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "       \n",
    "with open(\"naive_bayes_results.txt\", \"w\") as f:\n",
    "    # Write performance metrics\n",
    "    f.write(f\"Accuracy of the Naive Bayes classifier: {acc:.2f}\\n\")\n",
    "    f.write(f\"Error Rate: {err_rate:.2f}\\n\")\n",
    "    f.write(f\"Precision (weighted): {precision:.2f}\\n\")\n",
    "    f.write(f\"Recall (weighted): {recall:.2f}\\n\")\n",
    "    f.write(f\"F1-Score (weighted): {f1_score:.2f}\\n\")\n",
    "    f.write(f\"F2-Score (weighted, beta=2.0): {f2_score:.2f}\\n\")\n",
    "    f.write(f\"F0.5-Score (weighted, beta=0.5): {f0_5_score:.2f}\\n\")\n",
    "    f.write(f\"Specificity (weighted): {specificity:.2f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n--- All Test Predictions ---\\n\")\n",
    "    for i in range(len(X_test)):\n",
    "        sample_y_true = y_test[i]\n",
    "        sample_y_pred = y_pred[i]\n",
    "        if sample_y_pred != sample_y_true:\n",
    "            f.write(f\"Sample {i+1}: Predicted='{sample_y_pred}', Actual='{sample_y_true}'\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f83630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
